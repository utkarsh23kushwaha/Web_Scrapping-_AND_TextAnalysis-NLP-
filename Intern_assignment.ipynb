{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrLrER5Xvx3Q",
        "outputId": "23011fa2-7d54-49fc-c12d-16d377ee723f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#importing the required library\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vo_M5GmV3o96"
      },
      "outputs": [],
      "source": [
        "#script for extracting artciles from websites using Beautiful Soup and Requests\n",
        "\n",
        "#setting a user agent for acessing the webpages\n",
        "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'} \n",
        "\n",
        "#I have uploaded the INPUT_DATA sheet on google drive in order to access all the links using loop instead of scraping each of them manually\n",
        "input_data='https://drive.google.com/file/d/1x9l-mK_0wfNetqlZD_huOUWu1-egaC_m/view?usp=sharing'\n",
        "input_data='https://drive.google.com/uc?id=' + input_data.split('/')[-2]\n",
        "\n",
        "#parsing the INPUT DATA sheet as dataframe using pandas\n",
        "data = pd.read_csv(input_data)\n",
        "\n",
        "for i in range(len(data)):\n",
        "    url=data.at[i,'URL'] #this variable will be used to acess URLs\n",
        "    file_name=data.at[i,'URL_ID'] #this variable will be used to name the scraoed files\n",
        "\n",
        "    #this line is for skipping URLs with URL_ID : 44,57 and 144 as they are not working\n",
        "    if file_name==44 or file_name== 57 or file_name==144:\n",
        "        continue\n",
        "    else:\n",
        "        html = requests.get(f'{url}', headers=headers)\n",
        "\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')     #parsing the web pages\n",
        "\n",
        "        title = soup.find(\"h1\", class_=\"entry-title\").text #extracting titles of web pages\n",
        "\n",
        "        paragraphs = [p.text for p in soup.find_all(\"p\")]  #extracting each paragraph separately\n",
        "\n",
        "        f = open(f'{file_name}.txt', 'w',encoding=\"utf-8\") #creating a new file with name = URL_ID\n",
        "\n",
        "        f.write(title + '\\n\\n')                            #adding title to the file\n",
        "\n",
        "        for p in paragraphs:                               #starting a for loop to add all the paragraphs in the .txt file from the aticle\n",
        "            f.write(p + '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9F-C-1A38U9"
      },
      "outputs": [],
      "source": [
        "#adding custom stop words from .txt file\n",
        "#Creating an empty list which will later be filled with stopwords provided in .txt files of stop words folder\n",
        "\n",
        "stop_words_list = []\n",
        "\n",
        "for i in range(1,8):\n",
        "  with open(f'StopWords_{i}.txt', 'r',encoding='latin-1') as f:\n",
        "   \n",
        "# acessing items on each line of .txt files and adding them to a list of custom stop words\n",
        "    for line in f: \n",
        "      lines=line.strip()\n",
        "      stop_words_list.append(line.strip())\n",
        "\n",
        "# creating a temporary list to access special entries, example: 'AFGHANI  | Afghanistan ' \n",
        "temp=[] \n",
        "for word in stop_words_list:\n",
        "  if ' ' in word:\n",
        "    a=word.split('|')\n",
        "    for words in a:\n",
        "      temp.append(words)\n",
        "    stop_words_list.remove(word) #removing duplicates\n",
        "\n",
        "stop_words_list=stop_words_list+temp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using openpyxl library to create and update an excel sheet to load the analysis data\n",
        "\n",
        "excel = openpyxl.Workbook() #creating a workbook\n",
        "sheet = excel.active        #going to active sheet\n",
        "sheet.title= 'Output_Data'  #naming the sheet\n",
        "sheet.append(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "             'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "             'AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','SYLLABLE PER WORD', 'PERSONAL PRONOUNS',\n",
        "             'AVG WORD LENGTH'])  #adding required columns to the sheet\n",
        "input_data = 'https://drive.google.com/file/d/1x9l-mK_0wfNetqlZD_huOUWu1-egaC_m/view?usp=sharing' \n",
        "input_data = 'https://drive.google.com/uc?id=' + input_data.split('/')[-2]\n",
        "\n",
        "for i in range(len(data)):\n",
        "    url = data.at[i, 'URL']  # this variable will be used to acess URLs\n",
        "    file_name = data.at[i, 'URL_ID']  # this variable will be used to name the scraoed files\n",
        "\n",
        "    # parsing the INPUT DATA sheet as dataframe using pandas\n",
        "    data = pd.read_csv(input_data)\n",
        "    if file_name==44 or file_name==57 or file_name==144:\n",
        "      continue\n",
        "    else:\n",
        "\n",
        "      def count_pn(str):                       #function to count the total personal pronouns using refular expression (regex)\n",
        "          pattern = r'\\b(I|we|my|ours|us)\\b'\n",
        "          regex = re.compile(pattern, re.IGNORECASE)\n",
        "          return len(regex.findall(text))\n",
        "\n",
        "\n",
        "      # Get the list of stop words in English\n",
        "      stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "      with open(f'{file_name}.txt', 'r') as f:\n",
        "          # Read the contents of the file\n",
        "          text = f.read()\n",
        "          sentences = nltk.sent_tokenize(text)\n",
        "          tokens = nltk.word_tokenize(text)\n",
        "          # w = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "          words_after_stopwords_list = []\n",
        "          total_words = []\n",
        "      for w in tokens:\n",
        "          if w not in stop_words and w not in string.punctuation:\n",
        "              total_words.append(w)\n",
        "\n",
        "      # total word count after excluding words from stopwords class of NLTK\n",
        "      word_count = len(total_words)\n",
        "\n",
        "      # total count of personal nouns as indicated in the assignment\n",
        "      personal_noun_count = count_pn(text)\n",
        "      # total count of characters \n",
        "      count_char = 0\n",
        "      # total count of characters \n",
        "      count_char = 0\n",
        "      for w in total_words:\n",
        "          count_char += len(w)\n",
        "      average_word_length = count_char / len(total_words)\n",
        "\n",
        "      # calculating positive score, negative score, polarity and subjectivity scores\n",
        "\n",
        "      with open('negative-words.txt', 'r', encoding='latin-1') as neg_word:     # loading negative words\n",
        "          n_words = neg_word.read()   \n",
        "\n",
        "      with open('positive-words.txt', 'r', encoding='latin-1') as pos_word:     # loading positive words\n",
        "          p_words = pos_word.read()\n",
        "\n",
        "      words_after_stopwords_list = []\n",
        "\n",
        "      for w in tokens:\n",
        "          if w not in stop_words_list:\n",
        "              words_after_stopwords_list.append(w)\n",
        "\n",
        "      positive_words = nltk.word_tokenize(p_words)\n",
        "      negative_words = nltk.word_tokenize(n_words)\n",
        "      negative_score = 0\n",
        "      positive_score = 0\n",
        "\n",
        "      for word in words_after_stopwords_list:\n",
        "          if word in negative_words:\n",
        "              negative_score += 1\n",
        "          elif word in positive_words:\n",
        "              positive_score += 1\n",
        "\n",
        "      # polarity score\n",
        "      polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "      # subjectivity_score\n",
        "      subjectivity_score = (positive_score + negative_score) / ((len(words_after_stopwords_list)) + 0.000001)\n",
        "\n",
        "      complex_word_count = 0\n",
        "      total_syllable_count = 0\n",
        "\n",
        "\n",
        "      def syllable_count(word):                  #function to count syllables in a word\n",
        "          word = word.lower()\n",
        "          count = 0\n",
        "          vowels = 'aeiou'\n",
        "          for letter in word:\n",
        "              if letter in vowels:\n",
        "                  count += 1\n",
        "          if word.endswith('es') or word.endswith('ed'):\n",
        "              count -= 1\n",
        "          return count\n",
        "\n",
        "\n",
        "      def complex_word_checker(word):           #function to check if a word has more than 2 syllables\n",
        "          if syllable_count(word) > 2:\n",
        "              return True\n",
        "\n",
        "\n",
        "      # calculation of total syllable count and complex word count\n",
        "\n",
        "      for word in total_words:\n",
        "          total_syllable_count += syllable_count(word)\n",
        "          if complex_word_checker(word):\n",
        "              complex_word_count += 1\n",
        "\n",
        "      # calculation of percentae of complex words\n",
        "      percentage_complex_words = complex_word_count / len(total_words)\n",
        "\n",
        "      # calculation of syllable per word\n",
        "      syllable_per_word = total_syllable_count / len(total_words)\n",
        "\n",
        "      # calculating average sentence length \n",
        "      average_sentence_length = len(total_words) / len(sentences)\n",
        "      \n",
        "\n",
        "      # average no. of words per sentence\n",
        "      average_number_of_words_per_sentence = len(total_words) / len(sentences)\n",
        "      average_number_of_words_per_sentence = int(average_number_of_words_per_sentence)\n",
        "      # calculation of fog index\n",
        "      Fog_Index = 0.4 * (percentage_complex_words + average_sentence_length)         # calculating average sentence length \n",
        "      sheet.append([file_name,url,positive_score,negative_score,polarity_score,subjectivity_score,average_sentence_length,percentage_complex_words,Fog_Index,average_number_of_words_per_sentence,complex_word_count,word_count,syllable_per_word,personal_noun_count,average_word_length])\n",
        "\n",
        "\n",
        "excel.save('Output_Data.xlsx')                           #saving the excel file "
      ],
      "metadata": {
        "id": "maEu_Pc1_yOv"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}